%%
%% This is file `sample-acmlarge.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,journal,bibtex,acmlarge')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmlarge.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmlarge, review, manuscript, screen]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

%%
%% These commands are for a JOURNAL article.
\acmJournal{POMACS}
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Experimentation Platforms: Literature Review }

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Innocent Ndubuisi-Obi Jr}
\email{innoobi@cs.washington.edu}
\affiliation{%
  \institution{University of Washington}
  \city{Seattle}
  \state{Washington}
  \country{USA}
}

\author{Siya Kulkarni}
\email{siyak2@uw.edu}
\affiliation{%
  \institution{University of Washington}
  \city{Seattle}
  \state{Washington}
  \country{USA}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  A series of literature reviews on other experimentation platforms and how their features can contribute to dex.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Linkedin}
LinkedIn’s T-REX platform—short for Targeting, Ramping, and Experimentation—serves as the backbone of its A/B testing infrastructure, supporting up to 41,000 concurrent experiments across a user base of over 700 million members. Originally a modest system, T-REX evolved significantly over the past decade, driven by rapid company growth, increasing volumes of available data, and a cultural shift toward data-driven experimentation in product development.

The original experimentation system faced a number of serious limitations. Users were divided into 1,000 buckets based on user IDs, but this led to poor randomization and unreliable results. There was no centralized system for managing experiments, so test buckets were manually assigned through email, and test definitions were embedded directly in source code, making debugging tedious. Experiment deployment required time-consuming config updates, and results were manually compiled in spreadsheets or R, introducing inefficiencies and human error.

To meet the growing demands of more users and more simultaneous experiments, LinkedIn redesigned the system for scalability and usability. In the new model, a “T-REX Test” represents a hypothesis or feature under evaluation, while a “T-REX Experiment” refers to a specific stage in its rollout (e.g., 5\%, 25\%, 50\%). A centralized UI was introduced for managing and debugging tests, cutting deployment time to under five minutes and eliminating the need to change code for every new experiment. A key innovation was the Lix DSL—a domain-specific language that standardizes how experiments are defined.

The redesign also focused heavily on performance. A multi-tier caching system was introduced: the Lix client attempts to evaluate experiments from memory (with a ~98–99\% success rate), then falls back to a backend (~93\% success), and finally to Venice, LinkedIn’s key-value store. As a result, fewer than 0.2\% of requests ever hit persistent storage, greatly accelerating processing. The new experimentation engine is now 20 times faster and features better statistical methods and improved UI for data analysis. Additionally, LinkedIn has continued to refine its offline A/B testing pipelines, contributing to more accurate and scalable experimentation at every stage of development.

\section{Netflix} 
% Reference the authors, 
Netflix has redesigned its experimentation platform \cite{Diamantopoulos2019EngineeringFA} to be more science-centric, emphasizing scalability, performance, cost-efficiency, trustworthiness, usability, and extensibility. The platform standardizes on Python, R, and C++, which are languages familiar to data scientists and supported by extensive libraries—allowing for seamless development and execution of production-grade code in local environments. Since its reimagination, the platform has incorporated a wide range of advanced statistical methods, including quantile bootstrapping and regression models. This redesign enables engineers to devote more attention to improving the platform itself, contributing to long-term scalability and innovation.

Inspired by systems like Twitter's and Microsoft's ExP platform, Netflix sought to support a broad spectrum of metrics: built-in metrics tracked across all experiments, event-based metrics, and engineer-generated custom metrics. The Microsoft ExP framework in particular influenced Netflix’s focus on trustworthiness and scalability, especially through its four main components: the experimentation portal, execution service, log processing service, and analysis service. These components provide a foundation for deeper post-experiment analysis and metric interpretation.

To guide its overhaul, Netflix employed a rigorous research method that involved aggregating data from key meetings, identifying and categorizing potential design changes, and strategically staging these changes to reinforce architectural soundness. The revisions spanned several categories, including requirements gathering, software architecture updates, performance improvements, statistical method integration, and causal inference modeling.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{netflix_architecture.png}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

Architecturally, the new experimentation pipeline consists of three main phases: data collection, statistical analysis, and visualization. In the data collection phase, Netflix uses a centralized metrics repository developed in Python. Instead of conventional ETL pipelines, the system dynamically generates SQL that runs on Spark, allowing for fast execution and adaptable data processing. The statistical analysis phase leverages a causal models library implemented in Python, offering smooth integration with other statistical tools in R and Python. The visualization phase is powered by XP Viz, a library designed to translate statistical outputs into shareable visualizations, particularly well-suited for use in Jupyter Notebooks. The XP Platform API, a RESTful interface, stores experiment results and serves them to Ablaze (the frontend visualization tool) or directly to notebooks. To handle computational demand, Netflix utilizes OpenFaaS, a serverless computing framework that ensures the platform remains responsive and scalable under heavy workloads. Below is a graphic from Netflix that demonstrates how these components work together.

The impact of these architectural and methodological improvements has been significant. There has been a notable increase in metric contributions from across the company, and the integration of causal models has empowered Netflix to leverage Ordinary Least Squares (OLS) techniques, boosting statistical power. Additional capabilities such as segment discovery, quantile regression, and time series modeling have further expanded the platform’s analytical depth, allowing teams to draw more robust insights from experiments.

\section{Microsoft Flywheel}
Microsoft’s Experimentation Platform (ExP) team introduced the A/B Testing Flywheel as a practical framework for building and sustaining a strong experimentation culture within a company. The core idea is that scaling A/B testing takes time—it happens in stages, not all at once. The Flywheel outlines five key steps that reinforce each other: (1) running more A/B tests to support better decision-making, (2) measuring the value those tests bring, (3) increasing interest across the organization, (4) investing in infrastructure and data quality, and (5) lowering the effort and cost it takes to run experiments. With each full turn of the Flywheel, organizations build momentum and make experimentation a more natural part of how decisions get made.

\begin{figure}[ht]
    \centering
     \includegraphics[width=0.5\linewidth]{microsoft_flywheel.png}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

The first turn of the Flywheel is the hardest but most important. It starts with choosing experiments that are high-value, easy to measure, and simple to execute. Ideally, these first tests also produce results that are unexpected or challenge assumptions—what the authors call “counterintuitive” findings—which can spark curiosity and attention across the company. Alongside this, it’s essential to invest in reliable infrastructure early on, like automated data quality checks, A/A tests to ensure random assignment works, and shared tools to help teams get started quickly and confidently.

As the program matures, sharing insights becomes just as important as running tests. Regular newsletters, team-wide reviews, or presentations of standout experiments help spread knowledge and encourage more teams to participate. The platform itself should keep evolving—making it easier to define experiments, track metrics, and interpret results, especially for people who don’t have deep statistics backgrounds. Over time, it also becomes important to develop strong, well-defined metrics (like Overall Evaluation Criteria) that link test outcomes to business and customer value, and to add diagnostic metrics that help explain why a test succeeded or failed.

%what types of experiments are they supporting
%architecture, how its designed, how it works, who interacts w it

%microsoft exp has blog posts
%infrastructural experiemnts - closest to what we'll be doing

%key words: 
% keep an eye out for variance reduction techniques, spillover effects, interference, confounding factors

\section{Fabric Paper}
The FABRIC Measurement Framework (MF) provides a strong architectural blueprint for building a flexible and scalable measurement system within an experimental testbed. Its core goal is to support both infrastructure-level monitoring and experiment-specific instrumentation without interfering with the actual experiments. It works closely with the FABRIC Control Framework (CF), which handles resource provisioning, while MF focuses on enabling, managing, and analyzing measurement data.

There are 6 components that make up the MF. First is the Measurement Instrumentation and Control Service, which allows a user to take measurements and can enable and disable these experiments. Next is Measurement Points (MPs) that collect data about an experiment that is running, such as state information and events. Next is Timestamping Services, which MPs call on to accurately timestamp various events. The fourth component, Measurement Data Storage Services, stores the data that was collected, and can do so either locally on a rack, somewhere within the project, or globally. Next there is the Measurement Data Filtering/Processing Services, which filters and transforms the data into easy to view formats that can later be visualized and analyzed. The Experimenter Measurement Tools allow experimenters to perform various functions on their measurement data, such as analyzing, or searching through it. Finally, the Measurement Bus allows users to transport measurement data between various other components of the MF, and is implemented in various different ways. Below is a visualization of this architecture.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{FABRIC_diagram.png}
\end{figure}

The MF is used by both operators and experimenters, but experimenters are blocked from some sensitive information. MF components are usually specific to an experiment so they must be created specifically for that experiment. This is the job of the Measurement Instrumentation and Control Service. It leverages the CF

Diving more into the Measurement Points, they are divided into two categories: provisionable resources (like VMs attached to switch mirror ports for packet capture), and accessible metrics already exposed by running services (such as SNMP data or OS-level stats). MF includes a dedicated Instrumentation and Control Service that configures and deploys these measurement points, equipping them with tools like Prometheus exporters or ELK Beats as needed. This separation of concerns allows users to focus on experiment logic while MF handles the complexity of measurement setup.


One of MF’s most valuable features is its ability to provide precise, synchronized timestamps. Each FABRIC rack includes a GPS-disciplined clock that pushes accurate timing across nodes using the Precision Time Protocol (PTP). These clocks support sub-microsecond synchronization, which is crucial for accurately correlating events across a distributed environment. To make this functionality available inside virtual machines, MF uses the ptp\_kvm kernel module to expose the host’s synchronized clock to the guest OS, and then synchronizes NIC clocks using phc2sys. This setup allows even VMs with passed-through hardware to generate highly accurate timestamps.

The framework also isolates measurement traffic from experiment data through the Measurement Bus. This is implemented using different mechanisms depending on the type of data: Apache Kafka for logs and events, and Prometheus-sidecar pipelines for numeric metrics. When necessary, MF can provision dedicated Quality-of-Service (QoS) channels across the network to carry measurement traffic without impacting the experiment’s performance. This effectively creates a parallel measurement plane alongside the experiment’s data plane.

In terms of data storage and analysis, MF supports both local and centralized processing pipelines. Prometheus handles time-series metric data, which is aggregated using Thanos and visualized via Grafana. Logs and event data follow a Kafka → Logstash → Elasticsearch → Kibana (ELK) pipeline, offering advanced filtering and search capabilities. These pipelines can be deployed within FABRIC infrastructure or inside a user’s experiment slice, enabling users to apply the same toolchains to infrastructure and application-level data.
Another smart design choice is the way MF extends the CF information model. Measurement-related metadata—such as the presence of a GPS-synced clock or links to monitoring dashboards—are added as tags to existing resource objects. This allows MF to describe and manage measurement configurations without requiring structural changes to the CF’s resource model.

Security is built into the framework through role-based access controls and federated login (via CI-Logon and CoManage). All communications between components are encrypted using TLS and SASL, and access is restricted through ACLs to protect against external threats.

Users interact with MF through both web-based dashboards and APIs. Grafana and Kibana offer intuitive interfaces for exploring metrics and logs, while the API allows programmatic control over measurement workflows—such as enabling or disabling collection points, posting custom events, or querying stored data. This combination supports both interactive analysis and automated, large-scale experimentation.

FABRIC’s MF is a great reference when thinking about design choices for DEX. It demonstrates how to separate measurement from core experiment execution, integrate precise timing infrastructure, provide flexible and reusable data pipelines, and offer secure, user-friendly interfaces. The system has a strong structure and is designed very sensibly, making it an all-around great measurement platform.





\newpage

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}




%%
%% If your work has an appendix, this is the place to put it.
\appendix

\end{document}
\endinput
%%
%% End of file `sample-acmlarge.tex'.
