%%
%% This is file `sample-acmlarge.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,journal,bibtex,acmlarge')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmlarge.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmlarge, review, manuscript, screen]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

%%
%% These commands are for a JOURNAL article.
\acmJournal{POMACS}
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{8}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Experimentation Platforms: Literature Review }

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Innocent Ndubuisi-Obi Jr}
\email{innoobi@cs.washington.edu}
\affiliation{%
  \institution{University of Washington}
  \city{Seattle}
  \state{Washington}
  \country{USA}
}

\author{Siya Kulkarni}
\email{siyak2@uw.edu}
\affiliation{%
  \institution{University of Washington}
  \city{Seattle}
  \state{Washington}
  \country{USA}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  A series of literature reviews on other experimentation platforms and how their features can contribute to dex.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Linkedin}
\cite{linkedin}
LinkedIn’s T-REX platform—short for Targeting, Ramping, and Experimentation—serves as the backbone of its A/B testing infrastructure, supporting up to 41,000 concurrent experiments across a user base of over 700 million members. Originally a modest system, T-REX evolved significantly over the past decade, driven by rapid company growth, increasing volumes of available data, and a cultural shift toward data-driven experimentation in product development.

The original experimentation system faced a number of serious limitations. Users were divided into 1,000 buckets based on user IDs, but this led to poor randomization and unreliable results. There was no centralized system for managing experiments, so test buckets were manually assigned through email, and test definitions were embedded directly in source code, making debugging tedious. Experiment deployment required time-consuming config updates, and results were manually compiled in spreadsheets or R, introducing inefficiencies and human error.

To meet the growing demands of more users and more simultaneous experiments, LinkedIn redesigned the system for scalability and usability. In the new model, a “T-REX Test” represents a hypothesis or feature under evaluation, while a “T-REX Experiment” refers to a specific stage in its rollout (e.g., 5\%, 25\%, 50\%). A centralized UI was introduced for managing and debugging tests, cutting deployment time to under five minutes and eliminating the need to change code for every new experiment. A key innovation was the Lix DSL—a domain-specific language that standardizes how experiments are defined.

The redesign also focused heavily on performance. A multi-tier caching system was introduced: the Lix client attempts to evaluate experiments from memory (with a ~98–99\% success rate), then falls back to a backend (~93\% success), and finally to Venice, LinkedIn’s key-value store. As a result, fewer than 0.2\% of requests ever hit persistent storage, greatly accelerating processing. The new experimentation engine is now 20 times faster and features better statistical methods and improved UI for data analysis. Additionally, LinkedIn has continued to refine its offline A/B testing pipelines, contributing to more accurate and scalable experimentation at every stage of development.

\section{Netflix} 
% Reference the authors, 
Netflix has redesigned its experimentation platform \cite{netflix} to be more science-centric, emphasizing scalability, performance, cost-efficiency, trustworthiness, usability, and extensibility. The platform standardizes on Python, R, and C++, which are languages familiar to data scientists and supported by extensive libraries—allowing for seamless development and execution of production-grade code in local environments. Since its reimagination, the platform has incorporated a wide range of advanced statistical methods, including quantile bootstrapping and regression models. This redesign enables engineers to devote more attention to improving the platform itself, contributing to long-term scalability and innovation.

Inspired by systems like Twitter's and Microsoft's ExP platform, Netflix sought to support a broad spectrum of metrics: built-in metrics tracked across all experiments, event-based metrics, and engineer-generated custom metrics. The Microsoft ExP framework in particular influenced Netflix’s focus on trustworthiness and scalability, especially through its four main components: the experimentation portal, execution service, log processing service, and analysis service. These components provide a foundation for deeper post-experiment analysis and metric interpretation.

To guide its overhaul, Netflix employed a rigorous research method that involved aggregating data from key meetings, identifying and categorizing potential design changes, and strategically staging these changes to reinforce architectural soundness. The revisions spanned several categories, including requirements gathering, software architecture updates, performance improvements, statistical method integration, and causal inference modeling.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{netflix_architecture.png}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

Architecturally, the new experimentation pipeline consists of three main phases: data collection, statistical analysis, and visualization. In the data collection phase, Netflix uses a centralized metrics repository developed in Python. Instead of conventional ETL pipelines, the system dynamically generates SQL that runs on Spark, allowing for fast execution and adaptable data processing. The statistical analysis phase leverages a causal models library implemented in Python, offering smooth integration with other statistical tools in R and Python. The visualization phase is powered by XP Viz, a library designed to translate statistical outputs into shareable visualizations, particularly well-suited for use in Jupyter Notebooks. The XP Platform API, a RESTful interface, stores experiment results and serves them to Ablaze (the frontend visualization tool) or directly to notebooks. To handle computational demand, Netflix utilizes OpenFaaS, a serverless computing framework that ensures the platform remains responsive and scalable under heavy workloads. Below is a graphic from Netflix that demonstrates how these components work together.

The impact of these architectural and methodological improvements has been significant. There has been a notable increase in metric contributions from across the company, and the integration of causal models has empowered Netflix to leverage Ordinary Least Squares (OLS) techniques, boosting statistical power. Additional capabilities such as segment discovery, quantile regression, and time series modeling have further expanded the platform’s analytical depth, allowing teams to draw more robust insights from experiments.

\section{Microsoft ExP}
\cite{microsoft}
Microsoft’s ExP (Experimentation Platform) is designed to support large-scale, trustworthy A/B testing across a wide range of products, such as Bing, Microsoft Word, Windows OS, Xbox, and more. A key challenge the platform addresses is the need to support experimentation for highly diverse products with different architectures and technical constraints. It also has to function alongside decades-old codebases while maintaining compatibility and stability.

The two guiding values of ExP are trustworthiness and scalability. Trustworthiness ensures that results from experiments are reliable, statistically valid, and free from bias or interference. Scalability allows any product team, regardless of the platform they work on, to run trustworthy experiments with low overhead and to scale up their use of the platform as needed.

The experimentation portal is the main interface between experiment owners and the platform. It enables users to easily configure, launch, monitor, and analyze experiments. It includes tools for experiment management, metric definition, and result exploration. Within experiment management, users define the experiment audience, specify success criteria (called overall evaluation criteria or OEC), determine the size and duration of the experiment, and select from templates that encapsulate best practices like randomization techniques and ramp-up schedules. Experiment templates help minimize variance and ensure consistent implementation across teams. The system also addresses potential issues of variant interaction by grouping variants into isolation groups—mechanisms that ensure no user is accidentally exposed to conflicting variants from concurrent experiments. Owners must also configure how each variant behaves during the experiment, which can involve toggling features on or off using client-side configurations or adjusting server-side behavior.

Metrics are defined using a language developed at Microsoft called Metric Definition Language (MDL), which abstracts metric logic away from any specific backend system. This allows consistency across teams and products, since the same definition can later be compiled into SQL or other executable formats depending on the data processing stack. Users can test and validate metrics directly through the portal, ensuring accurate and consistent reporting.

For analyzing experiment results, the system generates an experiment scorecard. This is essentially a table that displays the performance of defined metrics across different experiment variants. The scorecard includes observed metric values for each variant, the differences between them, and the statistical significance of those differences. The portal also supports deeper visualizations, segmentation by user groups, time breakdowns, and custom analyses, helping teams interpret subtle or unexpected results. It also houses a searchable repository of past experiments to encourage cross-team learning and reduce duplicated efforts.

Experiment execution is handled by a service that assigns users to variants and delivers those assignments to the relevant application layer. Depending on the product, assignments can be delivered via direct service calls, through request annotations at edge nodes, or using local libraries. This flexibility allows the system to support both client-side and server-side experimentation.

Logs generated during experiments are collected and processed by a log processing service, which handles everything from real-time alerts to large-scale batch computations. Logs are “cooked,” or enriched and merged, so they can be easily consumed by analysis systems. Care is taken to ensure that no data is lost, including late-arriving client logs or offline activity. The system also actively monitors data quality to detect inconsistencies or telemetry failures that could distort results.

Throughout the entire experiment lifecycle, the analysis service plays a key role. Before an experiment begins, it assists in determining sample sizes, choosing a randomization seed, and evaluating potential bias. During the experiment, it monitors for harm to users and issues related to data integrity. Once the experiment concludes, it helps interpret the results, identifies heterogeneous treatment effects, and generates alerts or suggestions to guide decision-making. These capabilities are especially critical at Microsoft’s scale, where thousands of experiments are conducted annually and even small metric changes can have massive revenue implications.

In summary, ExP provides a comprehensive, end-to-end platform for experimentation that supports Microsoft’s diverse product ecosystem. It enables teams to quickly turn ideas into experiments and generate meaningful, trustworthy insights that help improve product quality and user experience.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{EXPERIMENTATION PORTAL.png}
\end{figure}


\section{Microsoft Flywheel}
Microsoft’s Experimentation Platform (ExP) team introduced the A/B Testing Flywheel as a practical framework for building and sustaining a strong experimentation culture within a company. The core idea is that scaling A/B testing takes time—it happens in stages, not all at once. The Flywheel outlines five key steps that reinforce each other: (1) running more A/B tests to support better decision-making, (2) measuring the value those tests bring, (3) increasing interest across the organization, (4) investing in infrastructure and data quality, and (5) lowering the effort and cost it takes to run experiments. With each full turn of the Flywheel, organizations build momentum and make experimentation a more natural part of how decisions get made.

\begin{figure}[ht]
    \centering
     \includegraphics[width=0.5\linewidth]{microsoft_flywheel.png}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

The first turn of the Flywheel is the hardest but most important. It starts with choosing experiments that are high-value, easy to measure, and simple to execute. Ideally, these first tests also produce results that are unexpected or challenge assumptions—what the authors call “counterintuitive” findings—which can spark curiosity and attention across the company. Alongside this, it’s essential to invest in reliable infrastructure early on, like automated data quality checks, A/A tests to ensure random assignment works, and shared tools to help teams get started quickly and confidently.

As the program matures, sharing insights becomes just as important as running tests. Regular newsletters, team-wide reviews, or presentations of standout experiments help spread knowledge and encourage more teams to participate. The platform itself should keep evolving—making it easier to define experiments, track metrics, and interpret results, especially for people who don’t have deep statistics backgrounds. Over time, it also becomes important to develop strong, well-defined metrics (like Overall Evaluation Criteria) that link test outcomes to business and customer value, and to add diagnostic metrics that help explain why a test succeeded or failed.

%what types of experiments are they supporting
%architecture, how its designed, how it works, who interacts w it

%microsoft exp has blog posts
%infrastructural experiemnts - closest to what we'll be doing

%key words: 
% keep an eye out for variance reduction techniques, spillover effects, interference, confounding factors

\section{Fabric Paper}


The FABRIC Measurement Framework (MF) provides a strong architectural blueprint for building a flexible and scalable measurement system within an experimental testbed. Its core goal is to support both infrastructure-level monitoring and experiment-specific instrumentation without interfering with the actual experiments. It works closely with the FABRIC Control Framework (CF), which handles resource provisioning, while MF focuses on enabling, managing, and analyzing measurement data.

There are 6 components that make up the MF. First is the Measurement Instrumentation and Control Service, which allows a user to take measurements and can enable and disable these experiments. Next is Measurement Points (MPs), which is any component or resource on the infrastructure that collects data about an experiment that is running, such as state information and events. Measurement points can either be Provisional Measurement Resources (PMRs), which are created specifically to run the measurement, or Accessible Meausrement Resources (AMRs), which are existing on the infrastructure already. Next is Timestamping Services, which MPs call on to accurately timestamp various events. The fourth component, Measurement Data Storage Services, stores the data that was collected, and can do so either locally on a rack, somewhere within the project, or globally. Next there is the Measurement Data Filtering/Processing Services, which filters and transforms the data into easy to view formats that can later be visualized and analyzed. The Experimenter Measurement Tools allow experimenters to perform various functions on their measurement data, such as analyzing, or searching through it. Finally, the Measurement Bus allows users to transport measurement data between various other components of the MF, and is implemented in various different ways. Below is a visualization of this architecture.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\linewidth]{FABRIC_diagram.png}
\end{figure}

The MF is used by both operators and experimenters, but experimenters are blocked from some sensitive information. MF components are usually specific to an experiment so they must be created specifically for that experiment. This is the job of the Measurement Instrumentation and Control Service. It leverages the CF

Diving more into the Measurement Points, they are divided into two categories: provisionable resources (like VMs attached to switch mirror ports for packet capture), and accessible metrics already exposed by running services (such as SNMP data or OS-level stats). MF includes a dedicated Instrumentation and Control Service that configures and deploys these measurement points, equipping them with tools like Prometheus exporters or ELK Beats as needed. This separation of concerns allows users to focus on experiment logic while MF handles the complexity of measurement setup.


One of MF’s most valuable features is its ability to provide precise, synchronized timestamps. Each FABRIC rack includes a GPS-disciplined clock that pushes accurate timing across nodes using the Precision Time Protocol (PTP). These clocks support sub-microsecond synchronization, which is crucial for accurately correlating events across a distributed environment. To make this functionality available inside virtual machines, MF uses the ptp\_kvm kernel module to expose the host’s synchronized clock to the guest OS, and then synchronizes NIC clocks using phc2sys. This setup allows even VMs with passed-through hardware to generate highly accurate timestamps.

The framework also isolates measurement traffic from experiment data through the Measurement Bus. This is implemented using different mechanisms depending on the type of data: Apache Kafka for logs and events, and Prometheus-sidecar pipelines for numeric metrics. When necessary, MF can provision dedicated Quality-of-Service (QoS) channels across the network to carry measurement traffic without impacting the experiment’s performance. This effectively creates a parallel measurement plane alongside the experiment’s data plane.

In terms of data storage and analysis, MF supports both local and centralized processing pipelines. Prometheus handles time-series metric data, which is aggregated using Thanos and visualized via Grafana. Logs and event data follow a Kafka → Logstash → Elasticsearch → Kibana (ELK) pipeline, offering advanced filtering and search capabilities. These pipelines can be deployed within FABRIC infrastructure or inside a user’s experiment slice, enabling users to apply the same toolchains to infrastructure and application-level data.
Another smart design choice is the way MF extends the CF information model. Measurement-related metadata—such as the presence of a GPS-synced clock or links to monitoring dashboards—are added as tags to existing resource objects. This allows MF to describe and manage measurement configurations without requiring structural changes to the CF’s resource model.

Security is built into the framework through role-based access controls and federated login (via CI-Logon and CoManage). All communications between components are encrypted using TLS and SASL, and access is restricted through ACLs to protect against external threats.

Users interact with MF through both web-based dashboards and APIs. Grafana and Kibana offer intuitive interfaces for exploring metrics and logs, while the API allows programmatic control over measurement workflows—such as enabling or disabling collection points, posting custom events, or querying stored data. This combination supports both interactive analysis and automated, large-scale experimentation.

FABRIC’s MF is a great reference when thinking about design choices for DEX. It demonstrates how to separate measurement from core experiment execution, integrate precise timing infrastructure, provide flexible and reusable data pipelines, and offer secure, user-friendly interfaces. The system has a strong structure and is designed very sensibly, making it an all-around great measurement platform.



\section{Kernels for Robust and Usable Experimentation}

\begin{table}[]
    \centering
    \begin{tabular}{cccc}
        \toprule
         \textbf{Pattern} & \textbf{Description} & \textbf{Examples} &  \\
         A/B Testing & Platform supports A/B Testing & Fabric, Microsoft, LinkedIn, Netflix & \\
         Scalability & Platform is usable for large amount of users/experiments & Fabric, Microsoft, Linkedin, Netflix & \\
         Centralized UI & User-friendly UI where experimenters can control and manage experiments& Fabric, Netflix \cite{netflix}, Microsoft \cite{microsoft}, Linkedin \cite{linkedin} & \\
         Domain-specific Language & Organization developed a DSL specifically for platform & LinkedIn \cite{linkedin} Microsoft \cite{microsoft}& \\
         Data Visualization & Experiment/Measurement results are visualized in meaningful ways & Fabric, Netflix \cite{netflix}& \\
         Results Analysis & Platform supports statistical analysis of results, accessible from central UI & Fabric, Netflix \cite{netflix}, Microsoft \cite{microsoft}& \\
         Security & Platform takes measures to ensure data is secure & Fabric, Microsoft, Linkedin& \\
         Preventing Overlap & Platform ensures diff variant of same experiment not assigned to same user & Microsoft \cite{microsoft} & \\
         Timestamping Service & Platform includes timestamping service to ensure ultra-accurate timings & Fabric & \\
         
         \midrule
         &&& 
         \bottomrule
    \end{tabular}
    \caption{Caption}
    \label{tab:my_label}
\end{table} \\

All 4 of the platforms reviewed so far share some basic patterns that inspiration can be drawn from when designing DEX Catalog. The three experimentation platforms reviewed—LinkedIn, Microsoft, Netflix—support A/B testing as a core capability, while Fabric's Measurement Framework is more involved with taking infrastructure measurements. Beyond A/B testing, the platforms, especiallly Microsoft and Netflix, support more complex experiments and statistical analysis, such as causal inference tools, segment discovery, and regression models to detect subtle treatment effects.

Each platform includes a centralized UI or control system to help users define, manage, and deploy experiments. For LinkedIn and Microsoft, this comes in the form of dedicated front-end portals, while FABRIC uses its Measurement Instrumentation and Control Service to handle measurement and experiment coordination, and also has a GUI. Regardless of the exact setup, these tools are designed to make experimentation smoother and more accessible across teams.

A common thread across all platforms is their focus on scalability, performance, and efficiency. Each system is built to handle large volumes of experiments and users, while maintaining low latency and high reliability. Some platforms have developed custom domain-specific languages to standardize and streamline experimentation. For example, LinkedIn introduced LiX DSL to define experiments in a consistent, reusable way, and Microsoft created MDL (Metric Definition Language) to describe metric logic independently of the language the backend is written in. These specific languages allow for reusable experiment templates, and standardized metric definitions.

Many of these platforms also include strong visualization components—either through custom dashboards or by pushing experiment data into tools like Jupyter Notebooks—so that teams can easily interpret and share their results. Fabric's MF uses Grafana and Kibana to do these visualizations, and Netflix uses a library called XP Viz as well as Ablaze. The platforms also support statistical analysis of their results, with Fabric using ES and Querier, Netflix using causal models written in Python, and Microsoft using their experiment scorecard.

\newpage

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}




%%
%% If your work has an appendix, this is the place to put it.
\appendix

\end{document}
\endinput
%%
%% End of file `sample-acmlarge.tex'.
